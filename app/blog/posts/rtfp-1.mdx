export const metadata = {
  title: 'Day 1',
  date: '2025-08-12',
  category: 'Road to My First Research Contribution',
};

Today marks my first day diving into AI interpretability research. Over the next year, my goal is to publish one official research paper, and for this, I'm starting a new "Road to My First Research Contribution" where I'll reflect on this journey.

I spent the day reading two papers: Towards a Rigorous Science of Interpretable Machine Learning and The Building Blocks of Interpretability. 

The first laid out the need for more precise definitions, taxonomies, and evaluation methods in interpretability. The second zoomed in on the building blocks of interpretability itself, critiquing the field’s tendency to explore methods in isolation rather than as composable, generalizable abstractions.

From these I took away something important: a lot of interpretability research is still fragmented. Many methods are useful in their own context but not easily combined with others. 

I noticed a few recurring themes and open questions that reallly excite me:

- How can we move from isolated techniques to abstractions that work across architectures, domains, and modalities?
- Is there such a thing as a “primary unit of attribution” in a neural network?
- Can we find functional equivalents of neurons across different models, and if so, what could we do with that knowledge?

My current note-taking system is split into 3 seperate pages:

1. Paper-specific notes will now include inline seeds-raw, informal thoughts written exactly where they occur to me while reading.
2. At the end of each paper, I’ll add a Connections section linking it to other papers, seeds, and knowledge notes.
3. The Research Seeds database will hold the polished version of each idea, ready for tagging, filtering, and novelty checks.

One of the biggest challenges I noticed today is that research reading isn’t just about understanding what a paper says. It’s about reading between the lines, spotting what the authors take for granted, noticing what they didn’t test, and catching assumptions that could be challenged. It's a whooooole other mental mode from reading a textbook. In a textbook, the goal is often to absorb the author’s explanations; in research, the goal is to interrogate them.
It’s also about crafting questions that live in a sweet spot: specific enough to be tested in an experiment, but general enough to matter beyond one dataset or architecture. That’s harder than it sounds!