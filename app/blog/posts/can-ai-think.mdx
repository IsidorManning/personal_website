export const metadata = {
  title: 'AI, Reasoning, Intelligence, and Thinking',
  date: '2025-08-12',
  category: 'Artificial Intelligence',
};

Does ChatGPT think? Does it reason? Does it think? is it actually intelligent? Why? Why not?

In this post I want to explore some thoughts about language models, reasoning, intelligence, and thinking. 

### Definitions

Let’s start with the classical definition of reasoning, intelligence, and thinking: 

reason: “the action of thinking about something in a logical, sensible way.”

intelligence: “the ability to acquire and apply knowledge and skills.”

To think: “have a particular opinion, belief, or idea about someone or something.”

## AI Reason vs. Human Reason: What’s the difference?

Lots of people say ”AI only imitate reasoning and doesn’t actually reasoning”. I used to say this. But recent language models are reasoning models. They think through and sort of lay out a ”mind map” of what to do in a logical way, before generating an output. This is standard practice now and almost all commercial language models are reasoning models. We, out course, also do this: if I want to create a new study plan I might follow a logical thinking path: first I need to look at my current strategy, then I might ask what’s working and what’s not working, then I need a place to write down the strategy, then I need to figure out my goals with this new study strategy, and so on. This is what reasoning models do, and so by definition, they are indeed reasoning.

Or are they just imitating what we do? We, the humans, reasoned in order to program the reasoning models to reason. So are they reasoning or just mechanically imitating how we reason? More importantly, what’s the difference our reasoning and an AI model’s reasoning? 

Maybe people say that they don’t ”truly reason” because these reasoning models lack other aspects of being human? For example, the models aren’t, as far as we know, conscious and so it doesn’t truly feel like they reason like we do. 

Secondly, one might say that they don’t have don’t reason because they can’t do it independently. They are not reasoning if they haven’t been prompted to do so. 

So again, what’s the difference between reasoning and following instructions to reason? Maybe none? Doesn’t we humans also follow ”instructions to reason?” We know that in order to reason, we, for example,  need to break down a problem into multiple sub-problems, then we need to use different tools and apply different bits of knowledge to solve these sub-problems, and finally we need to connect these sub-solutions all together to arrive at a final solution. This is one logical way to reason and I broke it down into instructions we can follow.

Maybe the difference is that the AI is prompted to reason by us? But isn’t that what we also do? We stumble upon a problem we need to solve or we are assigned a task we need to finish. Then we need to reason to solve the problem and finish the task. So we are also prompted to reason. i mean I wanted to explore this question and so I reason to do so. 

I think part of the answer lies in that AI models doesn’t yet feel human, and so we resort to thinking that they can’t reason, just because they aren’t humans. Maybe in the future when robotics have advanced more we will all agree that the robots reason, just because they are physical and more human-like.

So in many ways I think the answer of ”do AI reason” is yes, they do.

## Are AI models Intelligent?

Based on the definition of intelligence, AI models like large language models will have to be able to “acquire knowledge,” which means they need be able to obtain, apply, and develop knowledge. 

Language models might not have the same ability to independently develop and apply knowledge the same way we do with our brains. However, I think this is where we need to ask ourselves what we want from this technology. Sometimes people just point out what modern AI can’t do. A lot of people have said that ”AI can’t reason. It just predicts tokens and regurgitates stuff it learned in its training data.” There are countless examples of this that people will present: Some AI models would say 9.11 is bigger than 9.9, some multimodal ones couldn’t generate a full glass of whine, and some would try to predict the next entries in a sequence like ”plane, dog, apple” instead of saying that there is no pattern.

So what? If we really want AI to be general, intelligent, thinking, and reasoning, we need to develop the tools for it. We have already developed stuff like RAG and memory which allows for knowledge retrieval AI models can also use Python and other coding frameworks for precise calculations and data analysis, and recently OpenAI launched ChatGPT Agent where the model can browse and use the internet just like humans would. 

I think AI indeed is intelligent: they can apply knowledge when answering user prompts, the can obtain knowledge through reinforcement learning and fine-tuning, and they can develop knowledge through, for example, talking to the user and expanding their memory. 

In the future we will probably develop even more tools that make AI models more intelligent and integrated in our lives: advances in robotics could eventually result in physical AI, where an AI model can actually see you, live with you, and personalize itself based on your physical environment (more on that later). Long-term and short-term memory systems.

## Can AI think?

By definition, for an AI to ”think” it need to be able to ”have a particular opinion, belief, or idea about someone or something.” As of now, I don't think AI models think. They are intelligent, but they aren’t thinking. They reason to come up with an answer.

But at what point will they start to think? If we develop tools for AI to become more intelligent, at some point we might have to admit it is thinking. For example, if I had a personal, physical AI that walked around in my room, and it personalized and fine-tuned itself specifically to know me, then it could develop ideas of me, my persona, and my identity. It could learn from me, it inherent my beliefs and my thoughts, whether they are good or bad. 

With this we will have to debate a lot of questions in AI ethics. Like how do we decide what stuff is bad or good when something good for someone is bad for someone else? How do we algorithmically decide what an AI model should learn and inherent from the user? What information should it personalize itself to and refuse to internalize in its parameter space? 

Also, we will need to decide if we even want AI models to be able to think I’m the first place. Do we even want to develop AI systems that are able  to a develop beliefs and opinions? Would it be helpful and beneficial for humanity or would it be a catastrophic mistake?